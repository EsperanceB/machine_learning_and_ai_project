{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Harness the Power of [LangChain](https://python.langchain.com/en/latest/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queriesâ€”for example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain also includes components that allow LLMs to access new datasets without re-training. \n",
    "\n",
    "In this notebook, you explore how to use LangChain to develop applications powered by LLMs, harnessing LLM capabilities for various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Deploy text generation model.\n",
    "\n",
    "In this step, you use the SageMaker Python SDK to deploy the Falcon model for text generation. This permissively licensed ([Apache-2.0](https://jumpstart-cache-prod-us-east-2.s3.us-east-2.amazonaws.com/licenses/Apache-License/LICENSE-2.0.txt)) open source model is trained on the [RefinedWeb dataset](https://huggingface.co/datasets/tiiuae/falcon-refinedweb).\n",
    "\n",
    "<span style=\"color:red\">NOTE: The model might take 10 - 15 minutes to deploy.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:15:24.027436Z",
     "iopub.status.busy": "2025-06-30T13:15:24.026403Z",
     "iopub.status.idle": "2025-06-30T13:25:31.532101Z",
     "shell.execute_reply": "2025-06-30T13:25:31.531038Z",
     "shell.execute_reply.started": "2025-06-30T13:15:24.027394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'huggingface-llm-falcon-7b-instruct-bf16' with wildcard version identifier '*'. You can pin to version '4.6.7' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.base_predictor.Predictor at 0x7fb7cf12c650>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model_id = \"huggingface-llm-falcon-7b-instruct-bf16\"\n",
    "model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.2xlarge\")\n",
    "model.deploy(accept_eula=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Install LangChain and other required Python modules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:33:37.799842Z",
     "iopub.status.busy": "2025-06-30T13:33:37.799449Z",
     "iopub.status.idle": "2025-06-30T13:33:44.321055Z",
     "shell.execute_reply": "2025-06-30T13:33:44.319951Z",
     "shell.execute_reply.started": "2025-06-30T13:33:37.799817Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.12/site-packages (25.1.1)\n",
      "/bin/bash: line 1: 0.4: No such file or directory\n",
      "/bin/bash: line 1: 0.4: No such file or directory\n",
      "/bin/bash: line 1: 0.3: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip install --upgrade pip --root-user-action=ignore\n",
    "!pip install langchain < 0.4 --quiet --root-user-action=ignore\n",
    "!pip install --upgrade langchain_community < 0.4 --quiet --root-user-action=ignore\n",
    "!pip install --upgrade langchain-aws <0.3 --quiet --root-user-action=ignore\n",
    "!pip install faiss-cpu --quiet --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss is a library for efficient similarity searches and the clustering of dense vectors. Faiss is used for vector similarity searches in LangChain. To learn more, see the Faiss documentation at https://faiss.ai/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Import the required Python modules and set up the Amazon SageMaker runtime client.\n",
    "\n",
    "For more information about the Boto3 SageMakerRuntime client, see the documentation page at https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:33:56.903156Z",
     "iopub.status.busy": "2025-06-30T13:33:56.902716Z",
     "iopub.status.idle": "2025-06-30T13:33:56.967159Z",
     "shell.execute_reply": "2025-06-30T13:33:56.965830Z",
     "shell.execute_reply.started": "2025-06-30T13:33:56.903125Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "from typing import List\n",
    "\n",
    "session = boto3.Session()\n",
    "sagemaker_runtime_client = session.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use LangChain with an LLM hosted on a SageMaker endpoint to test a basic Q&A app. \n",
    "\n",
    "For more information about LangChain and SageMaker integration, see the documentation page at https://python.langchain.com/docs/integrations/llms/sagemaker.\n",
    "\n",
    "This code section sets up a question and answering chain by using a SageMaker endpoint as the LLM provider. The code defines a prompt template to be used with the LLM. The code also defines a custom content handler to handle input and output formatting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:34:27.176425Z",
     "iopub.status.busy": "2025-06-30T13:34:27.176014Z",
     "iopub.status.idle": "2025-06-30T13:34:27.669741Z",
     "shell.execute_reply": "2025-06-30T13:34:27.666596Z",
     "shell.execute_reply.started": "2025-06-30T13:34:27.176398Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain_aws import SagemakerEndpoint\n",
    "from langchain_aws.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "example_doc_1 = \"\"\"\n",
    "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from \n",
    "leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon via a single \n",
    "API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, \n",
    "and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, \n",
    "privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation \n",
    "(RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock \n",
    "is serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative \n",
    "AI capabilities into your applications using the AWS services you are already familiar with.\n",
    "\"\"\"\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=example_doc_1,\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        # print(input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        # print(response_json)\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 5: Test the Q&A chain with a sample question.\n",
    "\n",
    "This code block initializes the question and answering chain with the SageMaker endpoint and prompt template. The code invokes the chain with the example document and a sample question, and the code then prints the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:48:48.250914Z",
     "iopub.status.busy": "2025-06-30T13:48:48.250514Z",
     "iopub.status.idle": "2025-06-30T13:48:49.644933Z",
     "shell.execute_reply": "2025-06-30T13:48:49.643948Z",
     "shell.execute_reply.started": "2025-06-30T13:48:48.250814Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "\n",
      "[Document(metadata={}, page_content=\"\\nAmazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from \\nleading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon via a single \\nAPI, along with a broad set of capabilities you need to build generative AI applications with security, privacy, \\nand responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, \\nprivately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation \\n(RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock \\nis serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative \\nAI capabilities into your applications using the AWS services you are already familiar with.\\n\")]\n",
      "\n",
      "Question: What is Amazon Bedrock?\n",
      "Answer: Amazon Bedrock is a serverless service offering from Amazon that provides a range of AI-backed models from various vendors for use in high-performing generative AI applications.\n"
     ]
    }
   ],
   "source": [
    "embedding_endpoint_name = \"jumpstart-dft-hf-sentencesimilarity-20250630-133753\"\n",
    "instruct_endpoint_name = \"hf-llm-falcon-7b-instruct-bf16-2025-06-30-13-15-27-468\"\n",
    "\n",
    "parameters = {\n",
    "    \"max_length\": 200,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 1\n",
    "}\n",
    "\n",
    "prompt = PROMPT\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=instruct_endpoint_name,\n",
    "    region_name='us-east-1',\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "question = \"What is Amazon Bedrock?\"\n",
    "result = chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Use Retrieval Augmented Generation (RAG), with LangChain and SageMaker endpoints, to build a basic Q&A app.\n",
    "\n",
    "\n",
    "\n",
    "This practice section uses document embeddings to fetch the most relevant documents in the document knowledge library and combine the documents with the prompt that is provided to the LLM.\n",
    "\n",
    "To achieve this, you will:\n",
    "1. Generate embeddings for each document in the knowledge library by using the SageMaker GPT-J-6B embedding model.\n",
    "2. Identify the top K most relevant documents based on the user's query:\n",
    "    - 2.1 Generate the embedding of the query by using the same embedding model.\n",
    "    - 2.2 Search for the indexes of the top K most relevant documents in the embedding space by using an in-memory Faiss search.\n",
    "    - 2.3 Use the indexes to retrieve the corresponding documents.\n",
    "3. Combine the retrieved documents with the prompt and question, and send them to the SageMaker LLM.\n",
    "\n",
    "Note: The retrieved documents should be large enough to contain sufficient information to answer the question, but small enough to fit into the LLM prompt, which has a maximum sequence length of 1024 tokens.\n",
    "\n",
    "---\n",
    "To build the basic Q&A app with LangChain, you must:\n",
    "\n",
    "1. Wrap the SageMaker endpoints for the embedding model and the LLM into `langchain_community.embeddings.SagemakerEndpointEmbeddings` and `langchain_community.embeddings.sagemaker_endpoint`.\n",
    "\n",
    "2. Prepare the dataset to build the knowledge database.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:07:06.095350Z",
     "iopub.status.busy": "2025-06-30T14:07:06.094869Z",
     "iopub.status.idle": "2025-06-30T14:07:06.218910Z",
     "shell.execute_reply": "2025-06-30T14:07:06.217745Z",
     "shell.execute_reply.started": "2025-06-30T14:07:06.095312Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap the SageMaker endpoint for the embedding model into langchain_community.embeddings.SagemakerEndpointEmbeddings.\n",
    "# This code defines a custom subclass of SagemakerEndpointEmbeddings to handle document embeddings by using a SageMaker endpoint.\n",
    "# The code also defines a custom content handler for input and output formatting.\n",
    "\n",
    "from langchain_community.embeddings import SagemakerEndpointEmbeddings \n",
    "from langchain_community.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "\n",
    "\n",
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(self, texts: List[str], chunk_size: int = 5) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i : i + _chunk_size])\n",
    "            print\n",
    "            results.extend(response)\n",
    "        return results\n",
    "\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    " \n",
    "    def transform_input(self, text_inputs: List[str], model_kwargs: dict) -> bytes:\n",
    "        \"\"\"\n",
    "        Transforms the input into bytes that can be consumed by SageMaker endpoint.\n",
    "        Args:\n",
    "            text_inputs (list[str]): A list of input text strings to be processed.\n",
    "            model_kwargs (Dict): Additional keyword arguments to be passed to the endpoint.\n",
    "               Possible keys and their descriptions:\n",
    "               - mode (str): Inference method. Valid modes are 'embedding', 'nn_corpus', and 'nn_train_data'.\n",
    "               - corpus (str): Corpus for Nearest Neighbor. Required when mode is 'nn_corpus'.\n",
    "               - top_k (int): Top K for Nearest Neighbor. Required when mode is 'nn_corpus'.\n",
    "               - queries (list[str]): Queries for Nearest Neighbor. Required when mode is 'nn_corpus' or 'nn_train_data'.\n",
    "        Returns:\n",
    "            The transformed bytes input.\n",
    "        \"\"\"\n",
    "        input_str = json.dumps(\n",
    "            {\n",
    "                \"text_inputs\": text_inputs,\n",
    "                **model_kwargs\n",
    "            }\n",
    "        )\n",
    "        return input_str.encode(\"utf-8\")\n",
    " \n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Transforms the bytes output from the endpoint into a list of embeddings.\n",
    "        Args:\n",
    "            output: The bytes output from SageMaker endpoint.\n",
    "        Returns:\n",
    "            The transformed output - list of embeddings\n",
    "        Note:\n",
    "            The length of the outer list is the number of input strings.\n",
    "            The length of the inner lists is the embedding dimension.\n",
    "        \"\"\"\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "embeddings = SagemakerEndpointEmbeddingsJumpStart(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name='us-east-1',\n",
    "    model_kwargs={\"mode\": \"embedding\"},\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:07:16.716880Z",
     "iopub.status.busy": "2025-06-30T14:07:16.716462Z",
     "iopub.status.idle": "2025-06-30T14:07:16.725827Z",
     "shell.execute_reply": "2025-06-30T14:07:16.724498Z",
     "shell.execute_reply.started": "2025-06-30T14:07:16.716851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code wraps the SageMaker endpoint for the LLM into a SagemakerEndpoint object from LangChain.\n",
    "# The code also defines a custom content handler for input and output formatting.\n",
    "\n",
    "parameters = {\n",
    "    \"max_length\": 200,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 1\n",
    "}\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=instruct_endpoint_name,\n",
    "    region_name='us-east-1',\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.1 : Preprocess data \n",
    "Now, it's time to use the example data and prepare it for the demonstration. The knowledge library is provided by Amazon SageMaker FAQs at https://aws.amazon.com/sagemaker/faqs/. The data is formatted in a .csv file with two columns: Question and Answer. The Answer column is used for the documents of the knowledge library, from which relevant documents are retrieved based on a query. \n",
    "\n",
    "To build your own custom Q&A app, you can replace this example dataset with your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cases when you have data saved in multiple subsets, the following code reads all files that end with .csv and concatenates them together. Make sure each .csv file has the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:09:21.927812Z",
     "iopub.status.busy": "2025-06-30T14:09:21.927166Z",
     "iopub.status.idle": "2025-06-30T14:09:21.942193Z",
     "shell.execute_reply": "2025-06-30T14:09:21.941357Z",
     "shell.execute_reply.started": "2025-06-30T14:09:21.927759Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "all_files = glob.glob(os.path.join(\"rag_data/\", \"*.csv\"))\n",
    "\n",
    "df_knowledge = pd.concat(\n",
    "    (pd.read_csv(f, header=None, names=[\"Question\", \"Answer\"]) for f in all_files),\n",
    "    axis=0,\n",
    "    ignore_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the Question column, which is not used in this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:09:27.994259Z",
     "iopub.status.busy": "2025-06-30T14:09:27.993835Z",
     "iopub.status.idle": "2025-06-30T14:09:28.017893Z",
     "shell.execute_reply": "2025-06-30T14:09:28.016033Z",
     "shell.execute_reply.started": "2025-06-30T14:09:27.994230Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker is a fully managed service to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For a list of the supported Amazon SageMaker A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon SageMaker is designed for high availabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon SageMaker stores code in ML storage vol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon SageMaker ensures that ML model artifac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Answer\n",
       "0  Amazon SageMaker is a fully managed service to...\n",
       "1  For a list of the supported Amazon SageMaker A...\n",
       "2  Amazon SageMaker is designed for high availabi...\n",
       "3  Amazon SageMaker stores code in ML storage vol...\n",
       "4  Amazon SageMaker ensures that ML model artifac..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_knowledge.drop([\"Question\"], axis=1, inplace=True)\n",
    "df_knowledge.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 6.2: Use LangChain to use processed data and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:10:09.171064Z",
     "iopub.status.busy": "2025-06-30T14:10:09.170528Z",
     "iopub.status.idle": "2025-06-30T14:10:09.579693Z",
     "shell.execute_reply": "2025-06-30T14:10:09.578716Z",
     "shell.execute_reply.started": "2025-06-30T14:10:09.171031Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the necessary imports for building the Q&A app.\n",
    "from langchain.indexes.vectorstore import VectorstoreIndexCreator  \n",
    "from langchain_community.vectorstores.faiss import FAISS \n",
    "from langchain_text_splitters import CharacterTextSplitter \n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain_community.document_loaders import DataFrameLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LangChain to read the .csv file data. LangChain has multiple built-in functions to read different file formats, such as .txt, .html, and .pdf. For more information, see LangChain document loaders at https://python.langchain.com/docs/integrations/document_loaders/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:10:53.128164Z",
     "iopub.status.busy": "2025-06-30T14:10:53.127292Z",
     "iopub.status.idle": "2025-06-30T14:10:53.142438Z",
     "shell.execute_reply": "2025-06-30T14:10:53.141697Z",
     "shell.execute_reply.started": "2025-06-30T14:10:53.128120Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the processed data into LangChain.\n",
    "loader = DataFrameLoader(df_knowledge,page_content_column=\"Answer\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Q&A app with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the following question, you can achieve the points in Step 4 with just a few lines of code, as shown below the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:11:12.232745Z",
     "iopub.status.busy": "2025-06-30T14:11:12.232323Z",
     "iopub.status.idle": "2025-06-30T14:11:12.238787Z",
     "shell.execute_reply": "2025-06-30T14:11:12.237828Z",
     "shell.execute_reply.started": "2025-06-30T14:11:12.232715Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What is Amazon SageMaker?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:11:14.432923Z",
     "iopub.status.busy": "2025-06-30T14:11:14.432406Z",
     "iopub.status.idle": "2025-06-30T14:11:14.437474Z",
     "shell.execute_reply": "2025-06-30T14:11:14.436348Z",
     "shell.execute_reply.started": "2025-06-30T14:11:14.432887Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_creator = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embeddings,\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=300, chunk_overlap=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:11:30.132305Z",
     "iopub.status.busy": "2025-06-30T14:11:30.131993Z",
     "iopub.status.idle": "2025-06-30T14:11:31.763970Z",
     "shell.execute_reply": "2025-06-30T14:11:31.763222Z",
     "shell.execute_reply.started": "2025-06-30T14:11:30.132269Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = index_creator.from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:11:34.589039Z",
     "iopub.status.busy": "2025-06-30T14:11:34.588600Z",
     "iopub.status.idle": "2025-06-30T14:11:36.120107Z",
     "shell.execute_reply": "2025-06-30T14:11:36.119377Z",
     "shell.execute_reply.started": "2025-06-30T14:11:34.589005Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nAmazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.\\n\\nAmazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazonâ€™s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage.\\n\\nAmazon SageMaker includes built-in algorithms for linear regression, logistic regression, k-means clustering, principal component analysis, factorization machines, neural topic modeling, latent dirichlet allocation, gradient boosted trees, sequence2sequence, time-series forecasting, word2vec, and image classification. SageMaker also provides optimized Apache MXNet, Tensorflow, Chainer, PyTorch, Gluon, Keras, Horovod, Scikit-learn, and Deep Graph Library containers. In addition, Amazon SageMaker supports your custom training algorithms provided through a Docker image adhering to the documented specification.\\n\\nYou can get started with Amazon SageMaker Feature Store for free, as part of the\\xa0AWS Free Tier. With SageMaker Feature Store, you pay for writing into the feature store, and reading and storage from the online feature store. For pricing details, see the\\xa0SageMaker Pricing Page.\\n\\nQuestion: What is Amazon SageMaker?\\nHelpful Answer: Amazon SageMaker is a cloud-based service from Amazon that can prepare, train, and deploy machine learning and AI models that can be used to analyze data and make predictions or predictions based on data. \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(question=question, llm=sm_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Customize the previous Q&A app with a different prompt.\n",
    "\n",
    "You see how quickly LangChain can be used to create a question and answering application with just a few lines of code. You can break down the previous `VectorstoreIndexCreator` to see what's happening under the hood. You can also see how to use a custom prompt instead of a default prompt with `VectorstoreIndexCreator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate embeddings for each document in the knowledge library by using the SageMaker GPT-J-6B embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:12:27.836673Z",
     "iopub.status.busy": "2025-06-30T14:12:27.836292Z",
     "iopub.status.idle": "2025-06-30T14:12:28.669215Z",
     "shell.execute_reply": "2025-06-30T14:12:28.668387Z",
     "shell.execute_reply.started": "2025-06-30T14:12:27.836646Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:12:30.479152Z",
     "iopub.status.busy": "2025-06-30T14:12:30.478580Z",
     "iopub.status.idle": "2025-06-30T14:12:30.486488Z",
     "shell.execute_reply": "2025-06-30T14:12:30.485461Z",
     "shell.execute_reply.started": "2025-06-30T14:12:30.479104Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is Amazon SageMaker?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the previous question, you then identify the top K most relevant documents based on the user query, where K = 3 in this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:13:27.357703Z",
     "iopub.status.busy": "2025-06-30T14:13:27.357050Z",
     "iopub.status.idle": "2025-06-30T14:13:27.382715Z",
     "shell.execute_reply": "2025-06-30T14:13:27.381837Z",
     "shell.execute_reply.started": "2025-06-30T14:13:27.357674Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = docsearch.similarity_search(question, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top three most relevant documents, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:13:38.091747Z",
     "iopub.status.busy": "2025-06-30T14:13:38.091406Z",
     "iopub.status.idle": "2025-06-30T14:13:38.097488Z",
     "shell.execute_reply": "2025-06-30T14:13:38.096444Z",
     "shell.execute_reply.started": "2025-06-30T14:13:38.091721Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows. \n",
      "\n",
      "1: Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazonâ€™s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage. \n",
      "\n",
      "2: Amazon SageMaker includes built-in algorithms for linear regression, logistic regression, k-means clustering, principal component analysis, factorization machines, neural topic modeling, latent dirichlet allocation, gradient boosted trees, sequence2sequence, time-series forecasting, word2vec, and image classification. SageMaker also provides optimized Apache MXNet, Tensorflow, Chainer, PyTorch, Gluon, Keras, Horovod, Scikit-learn, and Deep Graph Library containers. In addition, Amazon SageMaker supports your custom training algorithms provided through a Docker image adhering to the documented specification. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for doc in docs:\n",
    "    print(f\"{n}: {doc.page_content} \\n\")\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, combine the retrieved documents with the prompt and question and send them to the SageMaker LLM.\n",
    "\n",
    "You define a customized prompt, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:14:35.123949Z",
     "iopub.status.busy": "2025-06-30T14:14:35.123659Z",
     "iopub.status.idle": "2025-06-30T14:14:35.129972Z",
     "shell.execute_reply": "2025-06-30T14:14:35.128999Z",
     "shell.execute_reply.started": "2025-06-30T14:14:35.123926Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:14:43.364108Z",
     "iopub.status.busy": "2025-06-30T14:14:43.363807Z",
     "iopub.status.idle": "2025-06-30T14:14:43.377401Z",
     "shell.execute_reply": "2025-06-30T14:14:43.368303Z",
     "shell.execute_reply.started": "2025-06-30T14:14:43.364083Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt=PROMPT\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the top three most relevant documents and question to the LLM to get a answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:14:53.071490Z",
     "iopub.status.busy": "2025-06-30T14:14:53.071047Z",
     "iopub.status.idle": "2025-06-30T14:14:53.264873Z",
     "shell.execute_reply": "2025-06-30T14:14:53.264074Z",
     "shell.execute_reply.started": "2025-06-30T14:14:53.071460Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the final answer from the LLM, as shown below, which is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:14:55.949676Z",
     "iopub.status.busy": "2025-06-30T14:14:55.948990Z",
     "iopub.status.idle": "2025-06-30T14:14:55.955051Z",
     "shell.execute_reply": "2025-06-30T14:14:55.953676Z",
     "shell.execute_reply.started": "2025-06-30T14:14:55.949643Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Answer based on context:\\n\\n[Document(id='3aec79fa-ab28-42b5-b3a1-c8ddb467ed6a', metadata={}, page_content='Amazon SageMaker is a fully managed service to prepare data and build, train, and deploy machine learning (ML) models for any use case with fully managed infrastructure, tools, and workflows.'), Document(id='d9a17c18-b8f5-43e6-9fc5-bf63e76b9654', metadata={}, page_content='Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazonâ€™s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS Region to provide fault tolerance in the event of a server failure or Availability Zone outage.'), Document(id='02adb5c6-cac4-4a6c-b257-c8bdefa8f891', metadata={}, page_content='Amazon SageMaker includes built-in algorithms for linear regression, logistic regression, k-means clustering, principal component analysis, factorization machines, neural topic modeling, latent dirichlet allocation, gradient boosted trees, sequence2sequence, time-series forecasting, word2vec, and image classification. SageMaker also provides optimized Apache MXNet, Tensorflow, Chainer, PyTorch, Gluon, Keras, Horovod, Scikit-learn, and Deep Graph Library containers. In addition, Amazon SageMaker supports your custom training algorithms provided through a Docker image adhering to the documented specification.')]\\n\\nWhat is Amazon SageMaker?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Apply additional use cases.\n",
    "\n",
    "Using LangChain and an LLM helps you create straightforward custom tools, such as code generators and chatbots.\n",
    "\n",
    "### Step 8.1 : Create a code generator with LangChain and an LLM.\n",
    "\n",
    "The following code snippet outlines the setup for automated code generation using LangChain and an LLM.\n",
    "\n",
    "- The code imports the necessary components from LangChain and initializes the LLM.\n",
    "- A prompt template is defined to guide the LLM in writing Python functions based on task descriptions.\n",
    "- A PromptTemplate instance is created, configuring how the task description is processed.\n",
    "- An LLMChain instance is then established, combining the prompt and the LLM, ready to generate code.\n",
    "- The system generates code by running the LLMChain with a given task description, demonstrating a streamlined method for converting task descriptions into functional Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:19:23.991381Z",
     "iopub.status.busy": "2025-06-30T14:19:23.990506Z",
     "iopub.status.idle": "2025-06-30T14:19:25.508168Z",
     "shell.execute_reply": "2025-06-30T14:19:25.506932Z",
     "shell.execute_reply.started": "2025-06-30T14:19:23.991346Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Write a Python function that takes a list of numbers and returns the sum of all even numbers.\n",
      "def sum_even_nums(nums):\n",
      "    sum = 0\n",
      "    for num in nums:\n",
      "        if num % 2 == 0:\n",
      "            sum += num\n",
      "    return sum\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Define the SageMaker LLM\n",
    "llm = sm_llm\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"\"\"\n",
    "Write a Python function that {task_description}.\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template instance\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"task_description\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create the LLMChain instance with the prompt and LLM\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "# Generate code based on the task description\n",
    "task_description = \"takes a list of numbers and returns the sum of all even numbers\"\n",
    "output = llm_chain.invoke(input=task_description)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 8.2: Create a chatbot with LangChain and an LLM.\n",
    "\n",
    "This code snippet demonstrates the process of establishing a conversation chain equipped with memory functionality, using LangChain and an LLM.\n",
    "\n",
    "- The code imports ConversationBufferMemory and ConversationChain from LangChain. The code then initializes the LLM by using a specific model.\n",
    "- The code creates a memory buffer designed to store and manage the context of a conversation, making sure past interactions can be recalled and used to inform an ongoing dialogue.\n",
    "- The final step involves constructing the ConversationChain, which seamlessly integrates both the LLM and the newly created memory buffer.\n",
    "\n",
    "Through this structured approach, the code facilitates the development of conversational AI interactions capable of retaining and using past dialogue. This capability significantly improves the interactions, making them more coherent and context-sensitive, thus mimicking a more natural and engaging conversational experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:21:02.457460Z",
     "iopub.status.busy": "2025-06-30T14:21:02.457058Z",
     "iopub.status.idle": "2025-06-30T14:21:02.501210Z",
     "shell.execute_reply": "2025-06-30T14:21:02.499993Z",
     "shell.execute_reply.started": "2025-06-30T14:21:02.457433Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574/1427672017.py:24: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"history\", return_messages=False)\n",
      "/tmp/ipykernel_574/1427672017.py:30: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import ConversationChain\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "parameters = { \n",
    "    \"max_length\": 200, \n",
    "    \"num_return_sequences\": 1, \n",
    "    \"top_k\": 10, \n",
    "    \"top_p\": 0.01, \n",
    "    \"do_sample\": False, \n",
    "    \"temperature\": 0, }\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=instruct_endpoint_name,\n",
    "    region_name='us-east-1',\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=content_handler,\n",
    ")\n",
    "\n",
    "# Initialize the LLM (in this case, OpenAI's GPT-3)\n",
    "llm = sm_llm\n",
    "\n",
    "# # Initialize the memory\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=False)\n",
    "\n",
    "# # Add system message\n",
    "memory.chat_memory.add_message(SystemMessage(content=\"You are a helpful professional assistant. Respond to the question only.\"))\n",
    "\n",
    "# Create the conversation chain with memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T14:21:42.550955Z",
     "iopub.status.busy": "2025-06-30T14:21:42.550107Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'm an AI assistant. How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Human:  Hi what is the weather like today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574/1813248441.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = conversation.run(input=human_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: You are a helpful professional assistant. Respond to the question only.\n",
      "Human: Hi what is the weather like today?\n",
      "AI: The temperature is pleasant, around 75 degrees Fahrenheit, and it's mostly Cloudy throughout the day.\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "def chat_with_ai():\n",
    "    print(\"Hi! I'm an AI assistant. How can I help you today?\")\n",
    "    try:\n",
    "        while True:\n",
    "            human_input = input(\"Human: \").strip()\n",
    "            if human_input.lower() == 'exit':\n",
    "                print(\"Assistant: Goodbye!\")\n",
    "                break  # Exit the loop if the user types 'exit'\n",
    "            # Process the input through the conversation chain using the run method.\n",
    "            response = conversation.run(input=human_input)\n",
    "            # Print the AI's response.\n",
    "            print(f\"Assistant: {response}\")\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nAssistant: Goodbye!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_with_ai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
